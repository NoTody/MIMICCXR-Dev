{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53bd3cee",
   "metadata": {},
   "source": [
    "rm -rf *.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4d5763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Train data length: 191027\n",
      "Valid data length: 202\n",
      "Train data length: 191027\n",
      "Valid data length: 202\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from barbar import Bar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)\n",
    "\n",
    "train_path = '/gpfs/data/denizlab/Datasets/Public/CheXpert-v1.0/train.csv'\n",
    "valid_path = '/gpfs/data/denizlab/Datasets/Public/CheXpert-v1.0/valid.csv'\n",
    "\n",
    "\n",
    "Traindata = pd.read_csv(train_path)\n",
    "Traindata = Traindata[Traindata['Path'].str.contains(\"frontal\")] # use only frontal images\n",
    "#Traindata = Traindata[500:]\n",
    "Traindata.to_csv('/gpfs/data/denizlab/Users/skr2369/Chexpert/CheXpert-v1/U1-V1/train_mod.csv', index = False)\n",
    "print(\"Train data length:\", len(Traindata))\n",
    "\n",
    "Validdata = pd.read_csv(valid_path)\n",
    "Validdata = Validdata[Validdata['Path'].str.contains(\"frontal\")] # use only frontal images\n",
    "Validdata.to_csv('/gpfs/data/denizlab/Users/skr2369/Chexpert/CheXpert-v1/U1-V1/valid_mod.csv', index = False)\n",
    "print(\"Valid data length:\", len(Validdata))\n",
    "\n",
    "# Testdata = Traindata.head(500) # use first 500 training data as test data (obs ratio is almost same!)\n",
    "# Testdata.to_csv('/gpfs/data/denizlab/Users/skr2369/Chexpert/CheXpert-v1/test_mod.csv', index = False)\n",
    "# print(\"Test data length:\", len(Testdata))\n",
    "\n",
    "pathFileTrain = '/gpfs/data/denizlab/Users/skr2369/Chexpert/CheXpert-v1/U1-V1/train_mod.csv'\n",
    "pathFileValid = '/gpfs/data/denizlab/Users/skr2369/Chexpert/CheXpert-v1/U1-V1/valid_mod.csv'\n",
    "# pathFileTest = '/gpfs/data/denizlab/Users/skr2369/Chexpert/CheXpert-v1/test_mod.csv'\n",
    "\n",
    "# Neural network parameters:\n",
    "nnIsTrained = False     # pre-trained using ImageNet\n",
    "nnClassCount = 14       # dimension of the output\n",
    "\n",
    "# Training settings: batch size, maximum number of epochs\n",
    "trBatchSize = 16\n",
    "trMaxEpoch = 3\n",
    "\n",
    "# Parameters related to image transforms: size of the down-scaled image, cropped image\n",
    "imgtransResize = (320, 320)\n",
    "imgtransCrop = 224\n",
    "\n",
    "# Class names\n",
    "class_names = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', \n",
    "               'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "               'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']\n",
    "\n",
    "class CheXpertDataSet(Dataset):\n",
    "    def __init__(self, data_PATH, transform = None, policy = \"ones\"):\n",
    "        \"\"\"\n",
    "        data_PATH: path to the file containing images with corresponding labels.\n",
    "        transform: optional transform to be applied on a sample.\n",
    "        Upolicy: name the policy with regard to the uncertain labels.\n",
    "        \"\"\"\n",
    "        image_names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(data_PATH, \"r\") as f:\n",
    "            csvReader = csv.reader(f)\n",
    "            next(csvReader, None) # skip the header\n",
    "            for line in csvReader:\n",
    "                image_name = line[0]\n",
    "                label = line[5:]\n",
    "                \n",
    "                for i in range(14):\n",
    "                    if label[i]:\n",
    "                        a = float(label[i])\n",
    "                        if a == 1:\n",
    "                            label[i] = 1\n",
    "                        elif a == -1:\n",
    "                            if policy == \"ones\":\n",
    "                                label[i] = 1\n",
    "                            elif policy == \"zeroes\":\n",
    "                                label[i] = 0\n",
    "                            else:\n",
    "                                label[i] = 0\n",
    "                        else:\n",
    "                            label[i] = 0\n",
    "                    else:\n",
    "                        label[i] = 0\n",
    "                \n",
    "                image_names.append('/gpfs/data/denizlab/Datasets/Public/' + image_name)\n",
    "        \n",
    "#                 image_names.append('./' + image_name)\n",
    "                labels.append(label)\n",
    "\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Take the index of item and returns the image and its labels\"\"\"\n",
    "        image_name = self.image_names[index]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.FloatTensor(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]  # mean of ImageNet dataset(for normalization)\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]   # std of ImageNet dataset(for normalization)\n",
    "\n",
    "# Tranform data\n",
    "normalize = transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
    "transformList = []\n",
    "\n",
    "transformList.append(transforms.Resize((imgtransCrop, imgtransCrop))) # 224\n",
    "# transformList.append(transforms.RandomResizedCrop(imgtransCrop))\n",
    "# transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "# transformList.append(normalize)\n",
    "transformSequence = transforms.Compose(transformList)\n",
    "\n",
    "# Load dataset\n",
    "datasetTrain = CheXpertDataSet(pathFileTrain, transformSequence, policy = \"ones\")\n",
    "print(\"Train data length:\", len(datasetTrain))\n",
    "dataLoaderTrain = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=20, pin_memory=True)\n",
    "\n",
    "\n",
    "datasetValid = CheXpertDataSet(pathFileValid, transformSequence)\n",
    "print(\"Valid data length:\", len(datasetValid))\n",
    "dataLoaderVal = DataLoader(dataset = datasetValid, batch_size = trBatchSize, \n",
    "                           shuffle = False, num_workers = 2, pin_memory = True)\n",
    "\n",
    "# data(\"Test data length:\", len(datasetTest))\n",
    "\n",
    "\n",
    "class CheXpertTrainer():\n",
    "\n",
    "    def train(model, dataLoaderTrain, dataLoaderVal, nnClassCount, trMaxEpoch, checkpoint):\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.0001, # setting optimizer & scheduler\n",
    "                               betas = (0.9, 0.999), eps = 1e-08, weight_decay = 0) \n",
    "        loss = torch.nn.BCELoss() # setting loss function\n",
    "        \n",
    "        if checkpoint != None and use_gpu: # loading checkpoint\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "            optimizer.load_state_dict(modelCheckpoint['optimizer'])\n",
    "            \n",
    "        # Train the network\n",
    "        lossMIN = 100000\n",
    "        train_start = []\n",
    "        train_end = []\n",
    "        for epochID in range(0, trMaxEpoch):\n",
    "            train_start.append(time.time()) # training starts\n",
    "            losst = CheXpertTrainer.epochTrain(model, dataLoaderTrain, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "            train_end.append(time.time()) # training ends\n",
    "#             lossv = CheXpertTrainer.epochVal(model, dataLoaderVal, optimizer, trMaxEpoch, nnClassCount, loss)\n",
    "            print(\"Training loss: {:.3f},\".format(losst))#, \"Valid loss: {:.3f}\".format(lossv))\n",
    "            \n",
    "            if losst < lossMIN:\n",
    "                lossMIN = losst\n",
    "                torch.save({'epoch': epochID + 1, 'state_dict': model.state_dict(), \n",
    "                            'best_loss': lossMIN, 'optimizer' : optimizer.state_dict()}, \n",
    "                           'm-epoch_FL' + str(epochID + 1) + '.pth.tar')\n",
    "                print('Epoch ' + str(epochID + 1) + ' [save] loss = ' + str(losst))\n",
    "            else:\n",
    "                print('Epoch ' + str(epochID + 1) + ' [----] loss = ' + str(losst))\n",
    "\n",
    "        train_time = np.array(train_end) - np.array(train_start)\n",
    "        print(\"Training time for each epoch: {} seconds\".format(train_time.round(0)))\n",
    "        params = model.state_dict()\n",
    "        return params\n",
    "       \n",
    "        \n",
    "    def epochTrain(model, dataLoaderTrain, optimizer, epochMax, classCount, loss):\n",
    "        losstrain = 0\n",
    "        model.train()\n",
    "\n",
    "        for batchID, (varInput, target) in enumerate(Bar(dataLoaderTrain)):\n",
    "            \n",
    "            varTarget = target.cuda(non_blocking = True)\n",
    "            varOutput = model(varInput)\n",
    "            lossvalue = loss(varOutput, varTarget)\n",
    "                       \n",
    "            optimizer.zero_grad()\n",
    "            lossvalue.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losstrain += lossvalue.item()\n",
    "            \n",
    "        return losstrain / len(dataLoaderTrain)\n",
    "\n",
    "    \n",
    "    def computeAUROC(dataGT, dataPRED, classCount):\n",
    "        # Computes area under ROC curve \n",
    "        # dataGT: ground truth data\n",
    "        # dataPRED: predicted data\n",
    "        outAUROC = []\n",
    "        datanpGT = dataGT.cpu().numpy()\n",
    "        datanpPRED = dataPRED.cpu().numpy()\n",
    "        \n",
    "        for i in range(classCount):\n",
    "            try:\n",
    "                outAUROC.append(roc_auc_score(datanpGT[:, i], datanpPRED[:, i]))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return outAUROC\n",
    "    \n",
    "    \n",
    "    def test(model, dataLoaderTest, nnClassCount, checkpoint, class_names):\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        if checkpoint != None and use_gpu:\n",
    "            modelCheckpoint = torch.load(checkpoint)\n",
    "            model.load_state_dict(modelCheckpoint['state_dict'])\n",
    "\n",
    "        if use_gpu:\n",
    "            outGT = torch.FloatTensor().cuda()\n",
    "            outPRED = torch.FloatTensor().cuda()\n",
    "        else:\n",
    "            outGT = torch.FloatTensor()\n",
    "            outPRED = torch.FloatTensor()\n",
    "       \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, (input, target) in enumerate(dataLoaderTest):\n",
    "\n",
    "                target = target.cuda()\n",
    "                outGT = torch.cat((outGT, target), 0).cuda()\n",
    "\n",
    "                bs, c, h, w = input.size()\n",
    "                varInput = input.view(-1, c, h, w)\n",
    "            \n",
    "                out = model(varInput)\n",
    "                outPRED = torch.cat((outPRED, out), 0)\n",
    "        aurocIndividual = CheXpertTrainer.computeAUROC(outGT, outPRED, nnClassCount)\n",
    "        aurocMean = np.array(aurocIndividual).mean()\n",
    "        print('AUROC mean ', aurocMean)\n",
    "        \n",
    "        for i in range (0, len(aurocIndividual)):\n",
    "            print(class_names[i], ' ', aurocIndividual[i])\n",
    "        \n",
    "        return outGT, outPRED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16124b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    \"\"\"Model modified.\n",
    "    The architecture of our model is the same as standard DenseNet121\n",
    "    except the classifier layer which has an additional sigmoid function.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained = False)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df07df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet model\n",
    "class resnet_model(nn.Module):\n",
    "    def __init__(self, size, features_dim, out_size, pretrained=False):\n",
    "        super(resnet_model, self).__init__()\n",
    "        \n",
    "        if size==18:\n",
    "            self.backbone = torchvision.models.resnet18(pretrained=pretrained)\n",
    "        elif size==50:\n",
    "            self.backbone = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        elif size==101:\n",
    "            self.backbone = torchvision.models.resnet101(pretrained=pretrained)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"ResNet with size {size} is not implemented!\")\n",
    "\n",
    "        #self.backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.feature_dim_in = self.backbone.fc.weight.shape[1]\n",
    "        self.backbone.fc = nn.Linear(in_features=self.feature_dim_in, out_features=features_dim, bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(features_dim, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18da27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 50\n",
    "features_dim = 2048\n",
    "out_size = 14\n",
    "\n",
    "model = resnet_model(size, features_dim, out_size, pretrained=True).cuda() # Step 0: Initialize global model and load the model\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "#model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecaa6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DenseNet121(nnClassCount).cuda() # Step 0: Initialize global model and load the model\n",
    "# model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "433d3d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance_Check.ipynb  \u001b[0m\u001b[38;5;9mm-epoch_FL2.pth.tar\u001b[0m         \u001b[38;5;9mm-epoch_FL_ResNet3.pth.tar\u001b[0m\r\n",
      "\u001b[38;5;33m__pycache__\u001b[0m/             \u001b[38;5;9mm-epoch_FL3.pth.tar\u001b[0m         \u001b[38;5;33mmodel_saved\u001b[0m/\r\n",
      "base3.py                 \u001b[38;5;9mm-epoch_FL_ResNet1.pth.tar\u001b[0m  train_mod.csv\r\n",
      "\u001b[38;5;9mm-epoch_FL1.pth.tar\u001b[0m      \u001b[38;5;9mm-epoch_FL_ResNet2.pth.tar\u001b[0m  valid_mod.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc9ee999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(load_path, model):\n",
    "    load_path = load_path\n",
    "    checkpoint = torch.load(load_path)\n",
    "    state_dict = {k.replace(\"img_backbone.\", \"module.\"): v for k, v in checkpoint['state_dict'].items()}\n",
    "    model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1606a9",
   "metadata": {},
   "source": [
    "# ResNet-50 Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc14f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.8034605977281252\n",
      "No Finding   0.9217657342657343\n",
      "Enlarged Cardiomediastinum   0.5199803632793324\n",
      "Cardiomegaly   0.7892156862745098\n",
      "Lung Opacity   0.9100050276520866\n",
      "Lung Lesion   0.37810945273631846\n",
      "Edema   0.9270833333333333\n",
      "Consolidation   0.8463235294117646\n",
      "Pneumonia   0.7899484536082474\n",
      "Atelectasis   0.8427296587926509\n",
      "Pneumothorax   0.8234432234432234\n",
      "Pleural Effusion   0.9321784420289854\n",
      "Pleural Other   0.8407960199004976\n",
      "Fracture   0.9234088457389428\n"
     ]
    }
   ],
   "source": [
    "path = \"m-epoch_FL_ResNet1.pth.tar\"\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8137b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.8029576408278938\n",
      "No Finding   0.8999125874125875\n",
      "Enlarged Cardiomediastinum   0.5264604810996564\n",
      "Cardiomegaly   0.8284313725490196\n",
      "Lung Opacity   0.9149321266968327\n",
      "Lung Lesion   0.23880597014925375\n",
      "Edema   0.9136904761904763\n",
      "Consolidation   0.8571691176470588\n",
      "Pneumonia   0.788659793814433\n",
      "Atelectasis   0.7938057742782153\n",
      "Pneumothorax   0.9025641025641025\n",
      "Pleural Effusion   0.9406702898550725\n",
      "Pleural Other   0.8955223880597015\n",
      "Fracture   0.9378248504462098\n"
     ]
    }
   ],
   "source": [
    "path1 = 'm-epoch_FL_ResNet2.pth.tar'\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path1, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c188789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.8625504258523878\n",
      "No Finding   0.8940122377622377\n",
      "Enlarged Cardiomediastinum   0.5771232204221896\n",
      "Cardiomegaly   0.821524064171123\n",
      "Lung Opacity   0.9180492709904474\n",
      "Lung Lesion   0.8656716417910448\n",
      "Edema   0.9230654761904762\n",
      "Consolidation   0.9033088235294118\n",
      "Pneumonia   0.8466494845360824\n",
      "Atelectasis   0.8371653543307086\n",
      "Pneumothorax   0.904029304029304\n",
      "Pleural Effusion   0.9266304347826088\n",
      "Pleural Other   0.8507462686567164\n",
      "Fracture   0.9451799548886928\n"
     ]
    }
   ],
   "source": [
    "path3 = 'm-epoch_FL_ResNet3.pth.tar'\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path3, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5bbdb",
   "metadata": {},
   "source": [
    "# CLIP ResNet50 trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52597064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.7830073622458903\n",
      "No Finding   0.8957604895104896\n",
      "Enlarged Cardiomediastinum   0.5549337260677467\n",
      "Cardiomegaly   0.8580659536541889\n",
      "Lung Opacity   0.8910005027652087\n",
      "Lung Lesion   0.05970149253731338\n",
      "Edema   0.919345238095238\n",
      "Consolidation   0.8836397058823529\n",
      "Pneumonia   0.8492268041237112\n",
      "Atelectasis   0.8086089238845144\n",
      "Pneumothorax   0.7648351648351648\n",
      "Pleural Effusion   0.9138360507246377\n",
      "Pleural Other   0.9054726368159204\n",
      "Fracture   0.8746690203000882\n"
     ]
    }
   ],
   "source": [
    "path = None\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5795df77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"model_saved/m-epoch_FL_resnet_clip2.pth.tar\"\n",
    "checkpoint = torch.load(load_path)\n",
    "state_dict = {k.replace(\"img_backbone.\", \"module.\"): v for k, v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "836d28f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.7961217992803559\n",
      "No Finding   0.8909527972027972\n",
      "Enlarged Cardiomediastinum   0.5240058910162003\n",
      "Cardiomegaly   0.7389705882352942\n",
      "Lung Opacity   0.8804424333836098\n",
      "Lung Lesion   0.4477611940298507\n",
      "Edema   0.921875\n",
      "Consolidation   0.8509191176470587\n",
      "Pneumonia   0.7197164948453608\n",
      "Atelectasis   0.8035695538057742\n",
      "Pneumothorax   0.8681318681318682\n",
      "Pleural Effusion   0.9099864130434783\n",
      "Pleural Other   0.8557213930348259\n",
      "Fracture   0.9375306462685103\n"
     ]
    }
   ],
   "source": [
    "path = None\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ca50633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"model_saved/m-epoch_FL_resnet_clip3.pth.tar\"\n",
    "checkpoint = torch.load(load_path)\n",
    "state_dict = {k.replace(\"img_backbone.\", \"module.\"): v for k, v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9554da85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.7299622005409355\n",
      "No Finding   0.8896416083916084\n",
      "Enlarged Cardiomediastinum   0.2927835051546392\n",
      "Cardiomegaly   0.7284982174688057\n",
      "Lung Opacity   0.893212669683258\n",
      "Lung Lesion   0.09452736318407962\n",
      "Edema   0.9226190476190476\n",
      "Consolidation   0.8628676470588235\n",
      "Pneumonia   0.7377577319587629\n",
      "Atelectasis   0.793490813648294\n",
      "Pneumothorax   0.8183150183150183\n",
      "Pleural Effusion   0.9093070652173914\n",
      "Pleural Other   0.6567164179104478\n",
      "Fracture   0.8897715014219869\n"
     ]
    }
   ],
   "source": [
    "path = None\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a58e9",
   "metadata": {},
   "source": [
    "# SLIP VICREG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcdd7c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"model_saved/m-epoch_FL_resnet_slipvicreg1.pth.tar\"\n",
    "checkpoint = torch.load(load_path)\n",
    "state_dict = {k.replace(\"img_backbone.\", \"module.\"): v for k, v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a0cd757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.7995279980480292\n",
      "No Finding   0.8533653846153846\n",
      "Enlarged Cardiomediastinum   0.4302405498281787\n",
      "Cardiomegaly   0.7995766488413548\n",
      "Lung Opacity   0.9102061337355455\n",
      "Lung Lesion   0.4328358208955224\n",
      "Edema   0.9375\n",
      "Consolidation   0.8398897058823529\n",
      "Pneumonia   0.8369845360824741\n",
      "Atelectasis   0.8015748031496063\n",
      "Pneumothorax   0.8219780219780219\n",
      "Pleural Effusion   0.9271965579710144\n",
      "Pleural Other   0.8656716417910448\n",
      "Fracture   0.9368441698538786\n"
     ]
    }
   ],
   "source": [
    "path = None\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ff3ff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"model_saved/m-epoch_FL_resnet_slipvicreg2.pth.tar\"\n",
    "checkpoint = torch.load(load_path)\n",
    "state_dict = {k.replace(\"img_backbone.\", \"module.\"): v for k, v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "836d28f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.7621250587463885\n",
      "No Finding   0.8830856643356644\n",
      "Enlarged Cardiomediastinum   0.504074619538537\n",
      "Cardiomegaly   0.8269830659536542\n",
      "Lung Opacity   0.9145299145299145\n",
      "Lung Lesion   0.30348258706467657\n",
      "Edema   0.9379464285714285\n",
      "Consolidation   0.8292279411764707\n",
      "Pneumonia   0.7725515463917526\n",
      "Atelectasis   0.8233070866141733\n",
      "Pneumothorax   0.780952380952381\n",
      "Pleural Effusion   0.9348958333333334\n",
      "Pleural Other   0.4626865671641791\n",
      "Fracture   0.9339021280768853\n"
     ]
    }
   ],
   "source": [
    "path = None\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99b2fdcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_path = \"model_saved/m-epoch_FL_resnet_slipvicreg3.pth.tar\"\n",
    "checkpoint = torch.load(load_path)\n",
    "state_dict = {k.replace(\"img_backbone.\", \"module.\"): v for k, v in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14023cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC mean  0.8185607501543342\n",
      "No Finding   0.9060314685314685\n",
      "Enlarged Cardiomediastinum   0.5668139420716741\n",
      "Cardiomegaly   0.8170677361853832\n",
      "Lung Opacity   0.8972347913524384\n",
      "Lung Lesion   0.5124378109452736\n",
      "Edema   0.9251488095238095\n",
      "Consolidation   0.8658088235294118\n",
      "Pneumonia   0.7222938144329897\n",
      "Atelectasis   0.8241469816272966\n",
      "Pneumothorax   0.8117216117216117\n",
      "Pleural Effusion   0.9296875\n",
      "Pleural Other   0.945273631840796\n",
      "Fracture   0.9176228302441894\n"
     ]
    }
   ],
   "source": [
    "path = None\n",
    "outGT, outPRED = CheXpertTrainer.test(model, dataLoaderVal, nnClassCount, path, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2df78ab",
   "metadata": {},
   "source": [
    "             Atelectasis       Cardiomegaly       Consolidation       Edema Pleural      Effusion\n",
    "PAPER:  0.858(0.806,0.910)  0.832(0.773,0.890)  0.899(0.854,0.944)  0.941(0.903,0.980)  0.934(0.901,0.967)\n",
    "Result:    0.85070              0.84569               0.88235              0.90595          0.92142\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
